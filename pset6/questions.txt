0.  According to a source cited by Wikipedia, Pneumonoultramicroscopicsilicovolcanoconiosis is "a word invented in imitation of polysyllabic medical terms, alleged to mean ‘a lung disease caused by the inhalation of very fine sand and ash dust (mostly volcanic silica ash dust)’ but occurring only as an instance of a very long word."
1.  getrusage() returns resource usage measures and statistics while the program is being run.
2.  16.
3.  We pass before and after by reference so that if or when the variables are modified outside of function calculate, it will still work. Moreover, also saves memory by not needing to create new variables.
4.  main first checks if there are the right number of arguments and returns 1 and the syntax if not. It then creates the structures to store time and baselines. After loading the dictionary, main opens up the text to spellcheck with fopen. Then go into a for loop, which says to run fgetc on the file each loop to get the info until main receives an end of file notification. Each loop runs an if statement to filter out nonalphabetical characters (excluding apostrophes) and, if true, adds the character to the word to be spellchecked (unless the alphabetical string is too long to be a word). If failed, it checks (via elseif) whether the character is a number and, if true, terminates the word. Finally, if variable word still has an index of over 0, that means we've reached the end of the word, so it terminates the current word, updates some benchmarks, and checks the spelling. After the for loop, checks for errors, closes the text file, determines size, unloads, and prints calculation results.
5.  relying on fscanf could cause problems with alphanumeric words.
6.  check and load are constant because we don't want them to be changed and so make them unrewritable.
7.  I used a hash table (a global array of nodes with the size equal to the possibilities I thought the hash could have) and a typedef'd struct node. Inside each node was a char array, "word", with a length just one bigger than max length so it would be able to store all (real) words. Each node also has a child node, "next", which essentially makes a linked list.
8.  My code was and is 11.96 seconds total.
9.  Given how many debuggings I ended up doing and how useless one office hour tf ended to be (tells me to fix something for reading, then later says I should be using fscanf anyway and to disregard many lines of code).
10. There is a massive bottleneck in check, as seen by the TIME IN statistics that it takes up the most time. Not really sure how to make more efficient. Maybe I should design a more efficient strcmp?
